#!/usr/bin/env python
import sys
import re
from time import time
from xmlrpclib import ServerProxy, MultiCall
import pickle
from datetime import datetime
from netways_ingraph import utils

print("NETWAYS inGraph (file collector)")

config = utils.load_config('ingraph-xmlrpc.conf')
config = utils.load_config('ingraph-aggregates.conf', config)
url = utils.get_xmlrpc_url(config)
api = ServerProxy(url, allow_none=True)

last_flush = time()
line_count = 0
ts_old = 0

intervals = []

tfs = api.getTimeFrames()
intervals = tfs.keys()
    
for aggregate in config['aggregates']:
    interval = aggregate['interval']
    
    if str(interval) in intervals:
        intervals.remove(str(interval))
    
    if 'retention-period' in aggregate:
        retention_period = aggregate['retention-period']
    else:
        retention_period = None
    
    api.setupTimeFrame(interval, retention_period)

for interval in intervals:
    tf = tfs[interval]
    print tf

updates = []

check_multi_regex = re.compile('^([^:]+::[^:]+)::([^:]+)$')

while  True:
    line = sys.stdin.readline()
        
    if not line:
        break;
    
    line_count += 1
    
    data = line.strip().split('\t')
    
    if len(data) < 4:
        continue
    elif len(data) < 5:
        data.append(time())
    
    logdata = {
        'host': data[0],
        'service': data[1],
        'text': data[2],
        'perf': data[3],
        'timestamp': int(data[4])
    }
    
    perfresults = utils.PerfdataParser.parse(logdata['perf'])
    
    is_multidata = False
    
    for plotname in perfresults:
        match = check_multi_regex.match(plotname)
                
        perfresult = perfresults[plotname]

        uom = perfresult['raw']['uom']
        raw_value = str(perfresult['raw']['value'])
        
        if 'warn' in perfresult:
            warn_lower = str(perfresult['warn']['lower']['value'])
            warn_upper = str(perfresult['warn']['upper']['value'])
            warn_type = str(perfresult['warn']['type'])
        else:
            warn_lower = None
            warn_upper = None
            warn_type = None            
        
        if 'crit' in perfresult:
            crit_lower = str(perfresult['crit']['lower']['value'])
            crit_upper = str(perfresult['crit']['upper']['value'])
            crit_type = str(perfresult['crit']['type'])
        else:
            crit_lower = None
            crit_upper = None
            crit_type = None

        if 'min' in perfresult:
            min_value = str(perfresult['min']['value'])
        else:
            min_value = None
        
        if 'max' in perfresult:
            max_value = str(perfresult['max']['value'])
        else:
            max_value = None

        upd_parentservice = None
        upd_service = logdata['service']
        upd_plotname = plotname

        if match:
            is_multidata = True
            
            multi_service = match.group(1)
            upd_plotname = match.group(2)

        if is_multidata:
            upd_parentservice = logdata['service']
            upd_service = multi_service

        update = (logdata['host'], upd_parentservice, upd_service, upd_plotname, logdata['timestamp'], \
                  uom, raw_value, raw_value, raw_value, min_value, max_value, warn_lower, warn_upper, warn_type, \
                  crit_lower, crit_upper, crit_type)
        updates.append(update)

    now = time()
    if last_flush + 30 < now or len(updates) >= 25000:
        updates_pickled = pickle.dumps(updates)
        st = time()
        api.insertValueBulk(updates_pickled)
        et = time()
        print("%d updates (%d lines) took %f seconds (%s -> %s)" % \
              (len(updates), line_count, et - st, datetime.fromtimestamp(ts_old), datetime.fromtimestamp(logdata['timestamp'])))
        updates = []
        line_count = 0
        last_flush = time()
        ts_old = logdata['timestamp']

if len(updates) > 0:
    updates_pickled = pickle.dumps(updates)
    st = time()
    api.insertValueBulk(updates_pickled)
    et = time()
    print("%d updates (%d lines) took %f seconds (%s -> %s)" % \
          (len(updates), line_count, et - st, datetime.fromtimestamp(ts_old), datetime.fromtimestamp(logdata['timestamp'])))
